import os
import chainlit as cl
from langchain.callbacks.base import BaseCallbackHandler
from langchain_core.messages import HumanMessage
import logging

# Import from modules
from src.utils.data_loader import load_data
from src.rag.vectorstore import get_vector_store
from src.models.llm import LLMManager
from src.models.workflow import WorkflowManager
from src.visualization.graph_visualizer import visualize_langgraph_workflow, visualize_simple_langgraph_workflow

# Configuration
CSV_PATH = "data/cleaned_pubmed_papers.csv"
VECTOR_STORE_PATH = "vectors/pubmed_vectors"
LLM_MODEL = "gemma3:4b"
OLLAMA_BASE_URL = "http://localhost:11434"

# ë¡œê¹… ë ˆë²¨ ì„¤ì • - í† í° ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ê³¼ë„í•œ ë¡œê¹… ë°©ì§€
cl.logger.setLevel(logging.INFO)

# ë‹¤ë¥¸ í¬íŠ¸ë¡œ ì‹¤í–‰í•˜ë ¤ë©´ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •: CHAINLIT_PORT=8001 chainlit run app.py

@cl.on_chat_start
async def on_chat_start():
    # Send initial message
    await cl.Message(
        content="ğŸ”¬ ìš°ìš¸ì¦ ê´€ë ¨ ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! í•™ìˆ ì  ë‚´ìš©ì´ë‚˜ ìƒë‹´ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
    ).send()
    
    # Load data
    try:
        df = load_data(CSV_PATH)
        cl.logger.info(f"CSV íŒŒì¼ì—ì„œ {len(df)}ê°œì˜ ë…¼ë¬¸ ë¡œë“œë¨")
    except Exception as e:
        await cl.Message(content=f"CSV ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}").send()
        return
    
    # Set up vector store
    with cl.Step("ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ ì¤‘...") as step:
        try:
            # Set vector_store path in the environment
            os.environ["VECTOR_STORE_PATH"] = VECTOR_STORE_PATH
            vector_store = get_vector_store(df)
            step.output = f"ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ ì™„ë£Œ (ë¬¸ì„œ {len(vector_store.index_to_docstore_id)}ê°œ)"
        except Exception as e:
            step.output = f"ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ ì‹¤íŒ¨: {str(e)}"
            await cl.Message(content="ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.").send()
            return
    
    # Initialize LLM
    with cl.Step("AI ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...") as step:
        try:
            llm_manager = LLMManager(
                model_name=LLM_MODEL,
                base_url=OLLAMA_BASE_URL,
                streaming=True
            )
            step.output = "AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ"
        except Exception as e:
            step.output = f"AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
            await cl.Message(content="AI ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.").send()
            return
    
    # Initialize workflow manager
    with cl.Step("ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì¤‘...") as step:
        try:
            workflow_manager = WorkflowManager(llm_manager, vector_store)
            step.output = "ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì™„ë£Œ"
            
            # ì›Œí¬í”Œë¡œìš° ì‹œê°í™”
            try:
                # ê¸°ì¡´ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ (ìˆìœ¼ë©´)
                viz_path = "visualization/simple_langgraph_workflow.png"
                if os.path.exists(viz_path):
                    os.remove(viz_path)
                    
                # ìƒˆ ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±
                graph_path = visualize_simple_langgraph_workflow()
                cl.logger.info(f"ìƒˆ ì›Œí¬í”Œë¡œìš° ì´ë¯¸ì§€ ìƒì„±ë¨: {graph_path}")
                
                elements = [
                    cl.Image(name="workflow_diagram", display="inline", path=graph_path)
                ]
                await cl.Message(content="LangGraph ì›Œí¬í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨:", elements=elements).send()
            except Exception as e:
                cl.logger.error(f"ì›Œí¬í”Œë¡œìš° ì‹œê°í™” ì‹¤íŒ¨: {str(e)}")
        except Exception as e:
            step.output = f"ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
            await cl.Message(content="ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.").send()
            return
    
    # Store workflow manager in user session
    cl.user_session.set("workflow_manager", workflow_manager)
    
    await cl.Message(content="ì¤€ë¹„ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.").send()

@cl.on_message
async def on_message(message: cl.Message):
    # Get question from user message
    question = message.content
    
    # Get workflow manager from user session
    workflow_manager = cl.user_session.get("workflow_manager")
    
    if not workflow_manager:
        await cl.Message(content="ì„¸ì…˜ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œê³ ì¹¨ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.").send()
        return
    
    # Create a streaming message
    msg = cl.Message(content="")
    await msg.send()
    
    # Custom streaming callback handler
    class ChainlitStreamingHandler(BaseCallbackHandler):
        def __init__(self):
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì´ ì‹œì‘ë˜ì—ˆëŠ”ì§€ ì¶”ì 
            self.streaming_started = False
        
        def on_llm_new_token(self, token: str, **kwargs):
            # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ í‘œì‹œ
            self.streaming_started = True
            # í† í°ì„ ì¦‰ì‹œ ìŠ¤íŠ¸ë¦¬ë°
            cl.run_sync(msg.stream_token(token))
    
    try:
        # ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬ ìƒì„±
        streaming_handler = ChainlitStreamingHandler()
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ë³€ìˆ˜ ì´ˆê¸°í™”
        documents = []
        question_type = "academic"  # ê¸°ë³¸ê°’
        last_node = None
        response_content = ""
        response_received = False
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ìƒíƒœ ì¶”ì 
        classify_done = False
        retrieve_done = False
        generation_started = False
        is_token_streaming = False
        
        # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        for state, metadata in workflow_manager.stream_process(
            question, 
            callbacks=[streaming_handler]
        ):
            # í˜„ì¬ ë…¸ë“œ ì´ë¦„
            current_node = metadata.get("langgraph_node", "unknown")
            
            # í† í° ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì—ëŠ” ë¡œê¹… ìµœì†Œí™”
            if streaming_handler.streaming_started and is_token_streaming:
                # í† í° ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ìœ ì§€
                continue
                
            # ìƒˆë¡œìš´ ë…¸ë“œë¡œ ì „í™˜ë  ë•Œ ìƒíƒœ ì—…ë°ì´íŠ¸
            if last_node != current_node:
                cl.logger.info(f"ë…¸ë“œ ì „í™˜: {last_node} -> {current_node}")
                last_node = current_node
                
                # ì§ˆë¬¸ ë¶„ë¥˜ ì™„ë£Œ
                if current_node == "classify" and not classify_done:
                    classify_done = True
                    if "question_type" in state:
                        question_type = state["question_type"]
                        cl.logger.info(f"ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ ì™„ë£Œ: {question_type}")
                
                # ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ
                elif current_node == "retrieve" and not retrieve_done:
                    retrieve_done = True
                    if "documents" in state and isinstance(state["documents"], list):
                        documents = state["documents"]
                        cl.logger.info(f"ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ ì°¾ìŒ")
                
                # ì‘ë‹µ ìƒì„± ì‹œì‘
                elif (current_node == "generate_academic" or current_node == "generate_counseling") and not generation_started:
                    generation_started = True
                    is_token_streaming = True
                    cl.logger.info(f"ì‘ë‹µ ìƒì„± ì‹œì‘: {current_node}")
            
            # ìƒíƒœì—ì„œ ì‘ë‹µ ë©”ì‹œì§€ í™•ì¸ - ìµœì¢… ì‘ë‹µ í™•ì¸
            if isinstance(state, dict) and "messages" in state:
                for msg_item in state["messages"]:
                    if not isinstance(msg_item, HumanMessage) and hasattr(msg_item, "content"):
                        message_content = msg_item.content
                        # íƒœê·¸ì— finalì´ ìˆëŠ”ì§€ í™•ì¸
                        is_final = hasattr(msg_item, "tags") and "final" in msg_item.tags
                        
                        # ìµœì¢… ì‘ë‹µì´ë©´ í† í° ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ í‘œì‹œ
                        if is_final:
                            is_token_streaming = False
                            cl.logger.info("ìµœì¢… ì‘ë‹µ ìˆ˜ì‹ ë¨")
                        
                        # ìƒˆ ì‘ë‹µ ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°
                        if message_content and message_content != response_content:
                            response_content = message_content
                            response_received = True
                            
                            # í† í° ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë°ì´ ì´ë¯¸ ì‘ë‹µì„ ì²˜ë¦¬ ì¤‘ì´ë©´ ìŠ¤í‚µ
                            if not streaming_handler.streaming_started:
                                cl.logger.info("í† í° ìŠ¤íŠ¸ë¦¬ë°ì´ ì‹œì‘ë˜ì§€ ì•Šì•„ ì „ì²´ ì‘ë‹µ ì—…ë°ì´íŠ¸")
                                msg.content = message_content
                                await msg.update()
        
        # í† í° ìŠ¤íŠ¸ë¦¬ë°ì´ ì„±ê³µì ìœ¼ë¡œ ì´ë£¨ì–´ì¡ŒëŠ”ì§€ í™•ì¸
        if streaming_handler.streaming_started:
            cl.logger.info("í† í° ìŠ¤íŠ¸ë¦¬ë°ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë¨")
        # ìŠ¤íŠ¸ë¦¬ë°ì´ ì—†ì—ˆì§€ë§Œ ì‘ë‹µì´ ìˆëŠ” ê²½ìš°
        elif response_received:
            cl.logger.info("ìŠ¤íŠ¸ë¦¬ë° ì—†ì´ ì „ì²´ ì‘ë‹µì´ ìˆ˜ì‹ ë¨")
            # ë©”ì‹œì§€ ì´ë¯¸ ì—…ë°ì´íŠ¸ë¨
        # ì‘ë‹µì´ ì „í˜€ ì—†ëŠ” ê²½ìš°
        else:
            cl.logger.warning("ì‘ë‹µì´ ìˆ˜ì‹ ë˜ì§€ ì•ŠìŒ, ì›Œí¬í”Œë¡œìš° ì‘ë‹µ ì§ì ‘ ì‹¤í–‰")
            
            # ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œë„
            try:
                # ë¹„ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì™„ì „í•œ ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°
                result = workflow_manager.process_message(question)
                
                # ì‘ë‹µ ì°¾ê¸°
                if isinstance(result, dict) and "messages" in result:
                    found_response = False
                    for msg_item in result["messages"]:
                        if not isinstance(msg_item, HumanMessage) and hasattr(msg_item, "content"):
                            found_response = True
                            msg.content = msg_item.content
                            await msg.update()
                            break
                    
                    if not found_response:
                        msg.content = "ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                        await msg.update()
                else:
                    msg.content = "ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                    await msg.update()
            except Exception as e:
                cl.logger.error(f"ì§ì ‘ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
                msg.content = "ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                await msg.update()
        
        # ë©”ì‹œì§€ê°€ ë¹„ì–´ ìˆëŠ” ê²½ìš° ê¸°ë³¸ ë©”ì‹œì§€ ì„¤ì •
        if not msg.content:
            msg.content = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
            await msg.update()
        
        # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ì¶”ê°€ ì •ë³´ í‘œì‹œ
        cl.logger.info(f"ìµœì¢… ì§ˆë¬¸ ìœ í˜•: {question_type}, ë¬¸ì„œ ìˆ˜: {len(documents)}")
        
        # í•™ìˆ  ì§ˆë¬¸ì˜ ê²½ìš° ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´ í‘œì‹œ
        if question_type == "academic" and documents:
            sources_text = "### ì°¸ê³  ë…¼ë¬¸\n\n"
            for i, doc in enumerate(documents[:3]):
                sources_text += f"**{i+1}.** ë…¼ë¬¸ ID: {doc.metadata.get('paper_id', 'N/A')}\n"
                sources_text += f"   ì œëª©: {doc.metadata.get('title', 'N/A')}\n"
                sources_text += f"   ì €ë„: {doc.metadata.get('journal', 'N/A')}\n\n"
            
            # ì†ŒìŠ¤ ì •ë³´ë¥¼ ìƒˆ ë©”ì‹œì§€ë¡œ ì „ì†¡
            await cl.Message(content=sources_text).send()
    
    except Exception as e:
        cl.logger.error(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        await cl.Message(content=f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}").send()
        msg.content = "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        await msg.update()
        return