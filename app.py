import os
import sys
import logging
import chainlit as cl
from langchain.callbacks.base import BaseCallbackHandler
from langchain_core.messages import HumanMessage, AIMessage
from langchain.globals import set_verbose
from langchain.callbacks.tracers.langchain import wait_for_all_tracers
from dotenv import load_dotenv

# src ë””ë ‰í† ë¦¬ë¥¼ ì‹œìŠ¤í…œ ê²½ë¡œì— ì¶”ê°€
if os.path.join(os.getcwd(), 'src') not in sys.path:
    sys.path.append(os.path.join(os.getcwd(), 'src'))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# LangSmith ì„¤ì • í™•ì¸ ë° í™œì„±í™”
if os.environ.get("LANGCHAIN_TRACING_V2") == "true":
    print("LangSmith ì¶”ì ì´ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    set_verbose(True)  # ìì„¸í•œ ë¡œê¹… í™œì„±í™”

# Tavily API í‚¤ í™•ì¸
tavily_api_key = os.environ.get("TAVILY_API_KEY")
if tavily_api_key:
    print("Tavily API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
else:
    print("ê²½ê³ : Tavily API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# Import from modules
from src.utils.data_loader import load_data
from src.rag.vectorstore import get_vector_store
from src.models.llm import LLMManager
from src.models.workflow import WorkflowManager
from src.visualization.graph_visualizer import visualize_langgraph_workflow,  visualize_current_workflow


# Configuration
CSV_PATH = "data/cleaned_pubmed_papers.csv"
VECTOR_STORE_PATH = "vectors/pubmed_vectors"
LLM_MODEL = "gemma3:4b"
OLLAMA_BASE_URL = "http://localhost:11434"

# ë‹¤ë¥¸ í¬íŠ¸ë¡œ ì‹¤í–‰í•˜ë ¤ë©´ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •: CHAINLIT_PORT=8001 chainlit run app.py

@cl.on_chat_start
async def on_chat_start():
    # Send initial message
    await cl.Message(
        content="ğŸ”¬ ìš°ìš¸ì¦ ê´€ë ¨ ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! í•™ìˆ ì  ë‚´ìš©ì´ë‚˜ ìƒë‹´ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
    ).send()
    
    # Load data
    try:
        df = load_data(CSV_PATH)
        cl.logger.info(f"CSV íŒŒì¼ì—ì„œ {len(df)}ê°œì˜ ë…¼ë¬¸ ë¡œë“œë¨")
    except Exception as e:
        await cl.Message(content=f"CSV ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}").send()
        return
    
    # Set up vector stores
    with cl.Step("ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ ì¤‘...") as step:
        try:
            # Set vector_store path in the environment
            os.environ["VECTOR_STORE_PATH"] = VECTOR_STORE_PATH
            
            # ë…¼ë¬¸ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
            vector_store = get_vector_store(df)
            cl.logger.info(f"ë…¼ë¬¸ ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ ì™„ë£Œ (ë¬¸ì„œ {len(vector_store.index_to_docstore_id)}ê°œ)")
            
            # ìœ íŠœë¸Œ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
            youtube_vector_path = "vectors/youtube_vectors"
            if os.path.exists(youtube_vector_path):
                from src.utils.youtube_embeddings import YoutubeEmbeddings
                youtube_embeddings = YoutubeEmbeddings()
                youtube_vector_store = youtube_embeddings.load_embeddings(youtube_vector_path)
                cl.logger.info(f"ìœ íŠœë¸Œ ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ ì™„ë£Œ")
            else:
                cl.logger.warning("ìœ íŠœë¸Œ ë²¡í„° ì €ì¥ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                youtube_vector_store = None
                
            step.output = f"ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ ì™„ë£Œ (ë…¼ë¬¸: {len(vector_store.index_to_docstore_id)}ê°œ, ìœ íŠœë¸Œ: {'ì‚¬ìš© ê°€ëŠ¥' if youtube_vector_store else 'ì—†ìŒ'})"
        except Exception as e:
            step.output = f"ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ ì‹¤íŒ¨: {str(e)}"
            await cl.Message(content="ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.").send()
            return
    # Initialize LLM
    with cl.Step("AI ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...") as step:
        try:
            llm_manager = LLMManager(
                model_name=LLM_MODEL,
                base_url=OLLAMA_BASE_URL,
                streaming=True
            )
            step.output = "AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ"
        except Exception as e:
            step.output = f"AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
            await cl.Message(content="AI ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.").send()
            return
    
    # Initialize workflow manager
    with cl.Step("ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì¤‘...") as step:
        try:
            workflow_manager = WorkflowManager(llm_manager, vector_store)
            step.output = "ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì™„ë£Œ"
            
            # ì›Œí¬í”Œë¡œìš° ì‹œê°í™”
            try:
                # ì „ì²´ ê²½ë¡œë¡œ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ì§€ì •
                project_root = os.getcwd()
                viz_dir = os.path.join(project_root, "visualization")
                viz_path = os.path.join(viz_dir, "langgraph_workflow.png")
                
                # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
                if not os.path.exists(viz_dir):
                    os.makedirs(viz_dir, exist_ok=True)
                    cl.logger.info(f"visualization ë””ë ‰í† ë¦¬ ìƒì„±ë¨: {viz_dir}")
                
                # ì´ë¯¸ì§€ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìƒˆë¡œ ìƒì„±í•˜ì§€ ì•Šê³  ê¸°ì¡´ ì´ë¯¸ì§€ ì‚¬ìš©
                if os.path.exists(viz_path) and os.path.getsize(viz_path) > 0:
                    cl.logger.info(f"ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° ì´ë¯¸ì§€ ì‚¬ìš©: {viz_path}")
                    graph_path = viz_path
                else:
                    # ì´ë¯¸ì§€ê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš° LangGraph ë‚´ì¥ ì‹œê°í™” ê¸°ëŠ¥ ì‚¬ìš©
                    cl.logger.info(f"LangGraph ë‚´ì¥ ì‹œê°í™” ê¸°ëŠ¥ìœ¼ë¡œ ì›Œí¬í”Œë¡œìš° ì´ë¯¸ì§€ ìƒì„± ì‹œì‘...")
                    graph_path = workflow_manager.visualize_workflow(viz_path)
                    if graph_path:
                        cl.logger.info(f"ìƒˆ ì›Œí¬í”Œë¡œìš° ì´ë¯¸ì§€ ìƒì„±ë¨: {graph_path}")
                    else:
                        # ì‹œê°í™” ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì‹œë„
                        cl.logger.warning("LangGraph ë‚´ì¥ ì‹œê°í™” ì‹¤íŒ¨, NetworkX ë°©ì‹ìœ¼ë¡œ ì‹œë„")
                        graph_path = visualize_current_workflow()
                        cl.logger.info(f"NetworkXë¡œ ìƒˆ ì›Œí¬í”Œë¡œìš° ì´ë¯¸ì§€ ìƒì„±ë¨: {graph_path}")
                
                # ì´ë¯¸ì§€ íŒŒì¼ ì¡´ì¬ í™•ì¸
                if os.path.exists(graph_path):
                    # ì´ë¯¸ì§€ í‘œì‹œ
                    elements = [
                        cl.Image(name="workflow_diagram", display="inline", path=graph_path)
                    ]
                    await cl.Message(content="LangGraph ì›Œí¬í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨:", elements=elements).send()
                    
                    # HTML íŒŒì¼ë„ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ ë§í¬ ì œê³µ
                    html_path = graph_path.replace(".png", ".html")
                    if os.path.exists(html_path):
                        # HTML íŒŒì¼ ê²½ë¡œì—ì„œ ìƒëŒ€ URL ìƒì„±
                        html_rel_path = os.path.relpath(html_path, project_root)
                        await cl.Message(content=f"[ì¸í„°ë™í‹°ë¸Œ ì›Œí¬í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨]({html_rel_path})").send()
                else:
                    cl.logger.error(f"ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {graph_path}")
                    await cl.Message(content="ì›Œí¬í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨ì„ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.").send()
            except Exception as e:
                cl.logger.error(f"ì›Œí¬í”Œë¡œìš° ì‹œê°í™” ì‹¤íŒ¨: {str(e)}")
                await cl.Message(content=f"ì›Œí¬í”Œë¡œìš° ì‹œê°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}").send()
        except Exception as e:
            step.output = f"ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
            await cl.Message(content="ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.").send()
            return
    
    # Store workflow manager in user session
    cl.user_session.set("workflow_manager", workflow_manager)
    
    await cl.Message(content="ì¤€ë¹„ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.").send()

@cl.on_message
async def on_message(message: cl.Message):
    # Get question from user message
    question = message.content
    
    # Get workflow manager from user session
    workflow_manager = cl.user_session.get("workflow_manager")
    
    if not workflow_manager:
        await cl.Message(content="ì„¸ì…˜ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œê³ ì¹¨ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.").send()
        return
    
    # Create a streaming message
    msg = cl.Message(content="")
    await msg.send()
    
    # Custom streaming callback handler
    class ChainlitStreamingHandler(BaseCallbackHandler):
        def __init__(self, message):
            self.message = message
            self.token_buffer = ""
            self.is_active = True
            print("ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”ë¨")
        
        def on_llm_start(self, serialized, prompts, **kwargs):
            print(f"LLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {len(prompts)} í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì¤‘")
        
        def on_llm_new_token(self, token: str, **kwargs):
            if not self.is_active:
                return
                
            try:
                if token:
                    #print(f"í† í° ìˆ˜ì‹ : {token[:10]}{'...' if len(token) > 10 else ''}")
                    self.token_buffer += token
                    # ë²„í¼ê°€ ì¶©ë¶„íˆ ì°¼ê±°ë‚˜ íŠ¹ì • ë¬¸ìê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì „ì†¡
                    if len(self.token_buffer) > 10 or any(c in self.token_buffer for c in ['\n', '.', '!', '?']):
                        cl.run_sync(self.message.stream_token(self.token_buffer))
                        self.token_buffer = ""
            except Exception as e:
                print(f"ìŠ¤íŠ¸ë¦¬ë° í† í° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                self.is_active = False
        
        def on_llm_end(self, response, **kwargs):
            # ë²„í¼ì— ë‚¨ì€ í† í° ëª¨ë‘ ì „ì†¡
            if self.token_buffer:
                try:
                    cl.run_sync(self.message.stream_token(self.token_buffer))
                except Exception as e:
                    print(f"ë§ˆì§€ë§‰ í† í° ì „ì†¡ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            print("LLM ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
        
        def on_llm_error(self, error, **kwargs):
            print(f"LLM ì˜¤ë¥˜ ë°œìƒ: {str(error)}")
            self.is_active = False
    
    try:
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬)
        last_state = None
        documents = []
        question_type = "academic"  # ê¸°ë³¸ê°’
        workflow_run_id = None      # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ID ì¶”ì ìš©
        
        # ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬ ìƒì„±
        streaming_handler = ChainlitStreamingHandler(msg)
        
        # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        for state, metadata in workflow_manager.stream_process(
            question, 
            callbacks=[streaming_handler]
        ):
            # ë…¸ë“œ ì´ë¦„
            node = metadata.get("langgraph_node", "unknown")
            
            # ì›Œí¬í”Œë¡œìš° ID ì¶”ì  (LangSmith í†µí•©ì„ ìœ„í•´)
            if workflow_run_id is None and "workflow_run_id" in metadata:
                workflow_run_id = metadata["workflow_run_id"]
                cl.logger.info(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ID: {workflow_run_id}")
            
            # ë‹¨ê³„ ì •ë³´ ë¡œê¹…
            cl.logger.info(f"ì›Œí¬í”Œë¡œìš° ë‹¨ê³„: {node}, Run ID: {metadata.get('workflow_run_id', 'N/A')}")
            
            # ì›Œí¬í”Œë¡œìš° ì§„í–‰ ì¤‘ì— ì •ë³´ ì¶”ì¶œ
            # ë¬¸ì„œ ì €ì¥ (retrieve ë…¸ë“œì—ì„œ)
            if "documents" in state and isinstance(state["documents"], list):
                documents = state["documents"]
            
            # ì§ˆë¬¸ ìœ í˜• ì €ì¥
            if "question_type" in state:
                question_type = state["question_type"]
                cl.logger.info(f"ì§ˆë¬¸ ìœ í˜• ê°±ì‹ : {question_type}")
                
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            last_state = state
                
        # ì‘ë‹µ ë©”ì‹œì§€ê°€ ìŠ¤íŠ¸ë¦¬ë°ë˜ì§€ ì•Šì€ ê²½ìš° ìµœì¢… ìƒíƒœì—ì„œ ì‘ë‹µ ì—…ë°ì´íŠ¸
        if not msg.content and last_state:
            # ë©”ì‹œì§€ ì°¾ê¸°
            ai_message_found = False
            for message in last_state.get("messages", []):
                if hasattr(message, "type") and message.type == "ai":
                    ai_message_found = True
                    msg.content = message.content
                    await msg.update()
                    break
                    
                # AIMessageì˜ ê²½ìš° í™•ì¸
                if not isinstance(message, HumanMessage) and hasattr(message, "content"):
                    ai_message_found = True
                    msg.content = message.content
                    await msg.update()
                    break
                    
            # ë©”ì‹œì§€ ê°ì²´ë¡œë¶€í„° ì§ì ‘ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì‹œë„
            if not ai_message_found and last_state.get("messages"):
                for message in last_state.get("messages"):
                    if hasattr(message, "content") and not isinstance(message, HumanMessage):
                        msg.content = message.content
                        await msg.update()
                        break
            
            # ì—¬ì „íˆ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€ ì„¤ì •
            if not msg.content:
                msg.content = "ì‘ë‹µì„ ìƒì„±í–ˆì§€ë§Œ ë‚´ìš©ì„ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                await msg.update()
                
        # ìµœì¢… ì§ˆë¬¸ ìœ í˜•ê³¼ ê²°ê³¼ ë¡œê¹…
        cl.logger.info(f"ìµœì¢… ì§ˆë¬¸ ìœ í˜•: {question_type}, ë¬¸ì„œ ìˆ˜: {len(documents)}, ì›Œí¬í”Œë¡œìš° ID: {workflow_run_id}")
        
        # í•™ìˆ  ì§ˆë¬¸ì— ëŒ€í•´ì„œë§Œ ì†ŒìŠ¤ ë¬¸ì„œ í‘œì‹œ
        if question_type == "academic" and documents:
            # íƒ€ë¹Œë¦¬ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸ (ì†ŒìŠ¤ê°€ "Tavily"ì¸ ë¬¸ì„œ)
            tavily_docs = [doc for doc in documents if doc.metadata.get("source") == "Tavily" or doc.metadata.get("source") == "Tavily Summary"]
            pubmed_docs = [doc for doc in documents if doc.metadata.get("source") != "Tavily" and doc.metadata.get("source") != "Tavily Summary"]
            
            # ì‹¤ì œ ë…¼ë¬¸ ì†ŒìŠ¤ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì°¸ê³  ë…¼ë¬¸ í‘œì‹œ
            if pubmed_docs and len(pubmed_docs) > 0:
                sources_text = "### ì°¸ê³  ë…¼ë¬¸\n\n"
                for i, doc in enumerate(pubmed_docs[:3]):
                    sources_text += f"**{i+1}.** ë…¼ë¬¸ ID: {doc.metadata.get('paper_id', 'N/A')}\n"
                    sources_text += f"   ì œëª©: {doc.metadata.get('title', 'N/A')}\n"
                    sources_text += f"   ì €ë„: {doc.metadata.get('journal', 'N/A')}\n\n"
                
                # ì†ŒìŠ¤ ì •ë³´ë¥¼ ìƒˆ ë©”ì‹œì§€ë¡œ ì „ì†¡
                sources_msg = cl.Message(content=sources_text)
                if workflow_run_id:
                    sources_msg.metadata = {"parent_run_id": workflow_run_id, "type": "sources"}
                await sources_msg.send()
            
            # íƒ€ë¹Œë¦¬ ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
            if tavily_docs:
                # íƒ€ë¹Œë¦¬ë§Œ ì‚¬ìš©í•œ ê²½ìš° ì„¤ëª… ì¶”ê°€
                if not pubmed_docs:
                    intro_text = "### ì›¹ ê²€ìƒ‰ ê²°ê³¼\n\nê´€ë ¨ ë…¼ë¬¸ì´ ë°ì´í„°ë² ì´ìŠ¤ì— ì—†ì–´ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n\n"
                else:
                    intro_text = "### ì›¹ ê²€ìƒ‰ ê²°ê³¼\n\n"
                
                tavily_text = intro_text
                
                # íƒ€ë¹Œë¦¬ ìš”ì•½ì„ ë¨¼ì € í‘œì‹œ
                summary_docs = [doc for doc in tavily_docs if doc.metadata.get("title") == "íƒ€ë¹Œë¦¬ ìš”ì•½"]
                search_result_docs = [doc for doc in tavily_docs if doc.metadata.get("title") != "íƒ€ë¹Œë¦¬ ìš”ì•½"]
                
                # ìš”ì•½ ì •ë³´ í‘œì‹œ
                if summary_docs:
                    summary_doc = summary_docs[0]
                    tavily_text += f"**AI ìš”ì•½:** {summary_doc.page_content}\n\n"
                    if search_result_docs:
                        tavily_text += "**ê²€ìƒ‰ ê²°ê³¼:**\n\n"
                
                # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
                for i, doc in enumerate(search_result_docs):
                    title = doc.metadata.get("title", "ê²€ìƒ‰ ê²°ê³¼")
                    url = doc.metadata.get("url", "")
                    
                    tavily_text += f"**{i+1}.** {title}\n"
                    if url:
                        tavily_text += f"   [ì›ë³¸ ë§í¬]({url})\n"
                    # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ë„ˆë¬´ ê¸¸ë©´ ìƒëµ)
                    content_preview = doc.page_content[:150] + "..." if len(doc.page_content) > 150 else doc.page_content
                    tavily_text += f"   {content_preview}\n\n"
                
                # íƒ€ë¹Œë¦¬ ì†ŒìŠ¤ ì •ë³´ ë©”ì‹œì§€ ì „ì†¡
                await cl.Message(content=tavily_text, author="ì›¹ ê²€ìƒ‰").send()
        
        # ìƒë‹´ ì§ˆë¬¸ì— ëŒ€í•œ ì†ŒìŠ¤ ë¬¸ì„œ í‘œì‹œ ì¶”ê°€
        elif question_type == "counseling" and documents:
            # ì†ŒìŠ¤ ë¬¸ì„œ í—¤ë” í‘œì‹œ
            await cl.Message(content="### ì°¸ê³  ìë£Œ", author="ì‹œìŠ¤í…œ").send()
            
            # ê° ë¬¸ì„œë¥¼ ê°œë³„ ë©”ì‹œì§€ë¡œ í‘œì‹œ
            for i, doc in enumerate(documents):
                title = doc.metadata.get("title", "ì œëª© ì—†ìŒ")
                url = doc.metadata.get("video_url", "")
                
                # ìœ íŠœë¸Œ URLì—ì„œ ë¹„ë””ì˜¤ ID ì¶”ì¶œ
                video_id = ""
                if url and "youtube.com" in url:
                    if "v=" in url:
                        video_id = url.split("v=")[1].split("&")[0]
                    elif "youtu.be/" in url:
                        video_id = url.split("youtu.be/")[1].split("?")[0]
                
                # ì œëª©ê³¼ ë§í¬ ë©”ì‹œì§€ ìƒì„±
                source_content = f"**{i+1}.** {title}\n\n"
                
                # ìœ íŠœë¸Œ URLì´ ìˆëŠ” ê²½ìš° ë§í¬ ì¶”ê°€
                if url:
                    source_content += f"[ìœ íŠœë¸Œ ì˜ìƒ ë°”ë¡œê°€ê¸°]({url})\n\n"
                
                # ë¹„ë””ì˜¤ ë¬¸ì„œ ë‚´ìš© ë©”ì‹œì§€ ì „ì†¡
                await cl.Message(content=source_content, author="ì‹œìŠ¤í…œ").send()
                
                # ìœ íŠœë¸Œ ì¸ë„¤ì¼ ì´ë¯¸ì§€ ë³„ë„ ì „ì†¡
                if video_id:
                    elements = []
                    # ìœ íŠœë¸Œ í”Œë ˆì´ì–´ ì¶”ê°€í•˜ë ¤ê³  ì‹œë„
                    try:
                        # ë¹„ë””ì˜¤ ìš”ì†Œ ì¶”ê°€ ì‹œë„
                        elements.append(
                            cl.Video(
                                name=f"youtube_video_{i}",
                                url=url,
                                display="inline"
                            )
                        )
                    except Exception as e:
                        # ë¹„ë””ì˜¤ ìš”ì†Œê°€ ì‹¤íŒ¨í•˜ë©´ ì´ë¯¸ì§€ë¡œ ëŒ€ì²´
                        thumbnail_url = f"https://img.youtube.com/vi/{video_id}/hqdefault.jpg"
                        elements.append(
                            cl.Image(
                                name=f"youtube_thumbnail_{i}",
                                url=thumbnail_url,
                                display="inline"
                            )
                        )
                    
                    # ë¹„ë””ì˜¤/ì´ë¯¸ì§€ ìš”ì†Œê°€ í¬í•¨ëœ ë©”ì‹œì§€ ë³„ë„ ì „ì†¡
                    await cl.Message(content="", author="ì‹œìŠ¤í…œ", elements=elements).send()
    
    except Exception as e:
        cl.logger.error(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        await cl.Message(content=f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}").send()
        msg.content = "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        await msg.update()
        return
    
    # ë©”ì‹œì§€ê°€ ì—¬ì „íˆ ë¹„ì–´ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
    if not msg.content:
        msg.content = "ì‘ë‹µ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆì§€ë§Œ ë‚´ìš©ì´ í‘œì‹œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        await msg.update()
        
    # LangSmith íŠ¸ë ˆì´ì„œê°€ ëª¨ë“  ìš”ì²­ì„ ë³´ë‚¼ ë•Œê¹Œì§€ ëŒ€ê¸°
    if os.environ.get("LANGCHAIN_TRACING_V2") == "true":
        try:
            await cl.make_async(wait_for_all_tracers)()
            cl.logger.info("ëª¨ë“  LangSmith íŠ¸ë ˆì´ìŠ¤ ì „ì†¡ ì™„ë£Œ")
        except Exception as e:
            cl.logger.error(f"LangSmith íŠ¸ë ˆì´ìŠ¤ ëŒ€ê¸° ì¤‘ ì˜¤ë¥˜: {str(e)}")