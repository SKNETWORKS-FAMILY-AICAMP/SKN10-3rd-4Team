import os
import chainlit as cl
from langchain.chains import RetrievalQA
from langchain.callbacks.base import BaseCallbackHandler

# Import from modules
from src.utils.data_loader import load_data
from src.rag.vectorstore import get_vector_store
from src.rag.prompts import PROMPT
from src.models.llm import LLMManager

# Configuration
CSV_PATH = "data/cleaned_pubmed_papers.csv"
VECTOR_STORE_PATH = "vectors/pubmed_vectors"
LLM_MODEL = "gemma3:4b"
OLLAMA_BASE_URL = "http://localhost:11434"

# ë‹¤ë¥¸ í¬íŠ¸ë¡œ ì‹¤í–‰í•˜ë ¤ë©´ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •: CHAINLIT_PORT=8001 chainlit run app.py

@cl.on_chat_start
async def on_chat_start():
    # Send initial message
    await cl.Message(
        content="ğŸ”¬ ìš°ìš¸ì¦ ê´€ë ¨ ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! í•™ìˆ ì  ë‚´ìš©ì´ë‚˜ ìƒë‹´ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
    ).send()
    
    # Load data
    try:
        df = load_data(CSV_PATH)
        cl.logger.info(f"CSV íŒŒì¼ì—ì„œ {len(df)}ê°œì˜ ë…¼ë¬¸ ë¡œë“œë¨")
    except Exception as e:
        await cl.Message(content=f"CSV ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}").send()
        return
    
    # Set up vector store
    with cl.Step("ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ ì¤‘...") as step:
        try:
            # Set vector_store path in the environment
            os.environ["VECTOR_STORE_PATH"] = VECTOR_STORE_PATH
            vector_store = get_vector_store(df)
            step.output = f"ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ ì™„ë£Œ (ë¬¸ì„œ {len(vector_store.index_to_docstore_id)}ê°œ)"
        except Exception as e:
            step.output = f"ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ ì‹¤íŒ¨: {str(e)}"
            await cl.Message(content="ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.").send()
            return
    
    # Initialize LLM
    with cl.Step("AI ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...") as step:
        try:
            llm_manager = LLMManager(
                model_name=LLM_MODEL,
                base_url=OLLAMA_BASE_URL,
                streaming=True
            )
            llm = llm_manager.llm
            step.output = "AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ"
        except Exception as e:
            step.output = f"AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
            await cl.Message(content="AI ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.").send()
            return
    
    # Create retrieval chain
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_store.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )
    
    # Store chain in user session
    cl.user_session.set("qa_chain", qa_chain)
    cl.user_session.set("retriever", vector_store.as_retriever(search_kwargs={"k": 3}))
    cl.user_session.set("llm_manager", llm_manager)
    
    await cl.Message(content="ì¤€ë¹„ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.").send()

@cl.on_message
async def on_message(message: cl.Message):
    # Get question from user message
    question = message.content
    
    # Get resources from user session
    qa_chain = cl.user_session.get("qa_chain")
    retriever = cl.user_session.get("retriever")
    llm_manager = cl.user_session.get("llm_manager")
    
    if not qa_chain or not retriever or not llm_manager:
        await cl.Message(content="ì„¸ì…˜ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œê³ ì¹¨ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.").send()
        return
    
    # Create a streaming message
    msg = cl.Message(content="")
    await msg.send()
    
    # Custom streaming callback handler
    class ChainlitStreamingHandler(BaseCallbackHandler):
        def on_llm_new_token(self, token: str, **kwargs):
            cl.run_sync(msg.stream_token(token))
    
    try:
        # ì§ˆë¬¸ ë¶„ë¥˜
        with cl.Step("ì§ˆë¬¸ ë¶„ë¥˜ ì¤‘...") as step:
            question_type = llm_manager.classify_question(question)
            step.output = f"ì§ˆë¬¸ ìœ í˜•: {'í•™ìˆ ì  ì§ˆë¬¸' if question_type == 'academic' else 'ìƒë‹´ ì§ˆë¬¸'}"
            
        # ìƒë‹´ ì§ˆë¬¸ì¸ ê²½ìš°
        if question_type == "counseling":
            with cl.Step("ìƒë‹´ ì‘ë‹µ ìƒì„± ì¤‘...") as step:
                # ìƒë‹´ ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
                response = llm_manager.generate_counseling_response(
                    question, 
                    callbacks=[ChainlitStreamingHandler()]
                )
                
                # ìŠ¤íŠ¸ë¦¬ë°ì´ ì‘ë™í•˜ì§€ ì•Šì€ ê²½ìš° ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
                if not msg.content:
                    msg.content = response
                    await msg.update()
                    
                step.output = "ìƒë‹´ ì‘ë‹µ ìƒì„± ì™„ë£Œ"
        
        # í•™ìˆ ì  ì§ˆë¬¸ì¸ ê²½ìš° 
        else:
            # Retrieve relevant documents
            with cl.Step("ê´€ë ¨ ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘...") as step:
                try:
                    docs = retriever.get_relevant_documents(question)
                    step.output = f"{len(docs)}ê°œì˜ ê´€ë ¨ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤."
                except Exception as e:
                    step.output = f"ë…¼ë¬¸ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"
                    await cl.Message(content="ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.").send()
                    return
            
            # Get context from documents for direct LLM response
            context = "\n\n".join([doc.page_content for doc in docs])
            
            with cl.Step("í•™ìˆ  ì‘ë‹µ ìƒì„± ì¤‘...") as step:
                # Use direct LLM call with streaming instead of chain
                # This ensures we see the tokens as they're generated
                response = await qa_chain.ainvoke(
                    {"query": question},
                    {"callbacks": [ChainlitStreamingHandler()]}
                )
                
                # If streaming didn't work, update the message with final result
                if not msg.content:
                    msg.content = response["result"]
                    await msg.update()
                
                step.output = "í•™ìˆ  ì‘ë‹µ ìƒì„± ì™„ë£Œ"
                
                # Display source documents for academic questions
                sources_text = "### ì°¸ê³  ë…¼ë¬¸\n\n"
                for i, doc in enumerate(response["source_documents"]):
                    sources_text += f"**{i+1}.** ë…¼ë¬¸ ID: {doc.metadata.get('paper_id', 'N/A')}\n"
                    sources_text += f"   ì œëª©: {doc.metadata.get('title', 'N/A')}\n"
                    sources_text += f"   ì €ë„: {doc.metadata.get('journal', 'N/A')}\n\n"
                
                # Send sources as a new message
                await cl.Message(content=sources_text).send()
    
    except Exception as e:
        await cl.Message(content=f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}").send()
        msg.content = "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        await msg.update()
        return
    
    # Update message if it's still empty
    if not msg.content:
        msg.content = "ì‘ë‹µ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆì§€ë§Œ ë‚´ìš©ì´ í‘œì‹œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        await msg.update()