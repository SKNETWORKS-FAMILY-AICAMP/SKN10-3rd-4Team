import os
import pandas as pd
import chainlit as cl
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.schema import Document

# Configuration
VECTOR_STORE_PATH = "vectors/pubmed_vectors"
CSV_PATH = "data/cleaned_pubmed_papers.csv"

# Prompt template
PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ì •ì‹ ì˜í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ PubMed ë…¼ë¬¸ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ì°¸ê³ í•  ë…¼ë¬¸ ë‚´ìš©:
{context}

ë‹µë³€ì€ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
1. ë‹µë³€ ë‚´ìš©ì„ ëª…í™•í•˜ê²Œ ì„¤ëª…
2. ì°¸ê³ í•œ ë…¼ë¬¸ ì •ë³´ ì–¸ê¸‰ (ì œëª©, ì €ë„)
3. ì •ë³´ê°€ ë¶ˆì¶©ë¶„í•œ ê²½ìš° ì •ì§í•˜ê²Œ ì¸ì •
"""

# Load CSV data
def load_data(csv_path):
    df = pd.read_csv(csv_path)
    cl.logger.info(f"CSV íŒŒì¼ì—ì„œ {len(df)}ê°œì˜ ë…¼ë¬¸ ë¡œë“œë¨")
    return df

# Get or create vector store
def get_vector_store(df):
    # Check if vector store already exists
    if os.path.exists(VECTOR_STORE_PATH) and os.path.isdir(VECTOR_STORE_PATH):
        try:
            # Initialize embeddings
            embeddings = OllamaEmbeddings(
                model="bge-m3",
                base_url="http://localhost:11434"
            )
            
            # Load existing vector store
            vector_store = FAISS.load_local(VECTOR_STORE_PATH, embeddings, allow_dangerous_deserialization=True)
            
            # Check document count
            doc_count = len(vector_store.index_to_docstore_id)
            cl.logger.info(f"ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œì—ì„œ {doc_count}ê°œ ë¬¸ì„œ ë°œê²¬")
            
            if doc_count == len(df):
                cl.logger.info("ë²¡í„° ì €ì¥ì†Œê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤. ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return vector_store
            else:
                cl.logger.info(f"ë²¡í„° ì €ì¥ì†Œ({doc_count}ê°œ)ì™€ CSV({len(df)}ê°œ)ì˜ ë¬¸ì„œ ìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤.")
                cl.logger.info("ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        except Exception as e:
            cl.logger.error(f"ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            cl.logger.info("ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
    else:
        cl.logger.info("ë²¡í„° ì €ì¥ì†Œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
    
    # Create new vector store
    cl.logger.info("ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
    
    # Create document objects
    documents = []
    for i, row in df.iterrows():
        content = f"Title: {row['title']}\n\nAbstract: {row['abstract']}"
        doc = Document(
            page_content=content,
            metadata={
                "paper_id": row["paper_id"],
                "paper_number": row["paper_number"],
                "journal": row["journal"],
                "title": row["title"]
            }
        )
        documents.append(doc)
    
    # Initialize embeddings
    embeddings = OllamaEmbeddings(
        model="bge-m3",
        base_url="http://localhost:11434"
    )
    
    # Create vector store
    vector_store = FAISS.from_documents(documents, embeddings)
    
    # Save vector store
    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)
    vector_store.save_local(VECTOR_STORE_PATH)
    cl.logger.info(f"{len(documents)}ê°œ ë¬¸ì„œì˜ ë²¡í„° ì €ì¥ì†Œê°€ '{VECTOR_STORE_PATH}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return vector_store

@cl.on_chat_start
async def on_chat_start():
    # Send initial message
    await cl.Message(
        content="ğŸ”¬ PubMed ë…¼ë¬¸ ê²€ìƒ‰ ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! ì •ì‹ ì˜í•™ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
    ).send()
    
    # Load data
    try:
        df = load_data(CSV_PATH)
    except Exception as e:
        await cl.Message(content=f"CSV ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}").send()
        return
    
    # Set up vector store
    with cl.Step("ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ ì¤‘...") as step:
        try:
            vector_store = get_vector_store(df)
            step.output = f"ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ ì™„ë£Œ (ë¬¸ì„œ {len(vector_store.index_to_docstore_id)}ê°œ)"
        except Exception as e:
            step.output = f"ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ ì‹¤íŒ¨: {str(e)}"
            await cl.Message(content="ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.").send()
            return
    
    # Initialize LLM
    with cl.Step("AI ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...") as step:
        try:
            llm = Ollama(
                model="gemma3:4b",
                base_url="http://localhost:11434",
                temperature=0.1
            )
            step.output = "AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ"
        except Exception as e:
            step.output = f"AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
            await cl.Message(content="AI ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.").send()
            return
    
    # Create prompt template
    prompt = PromptTemplate(
        template=PROMPT_TEMPLATE,
        input_variables=["context", "question"]
    )
    
    # Create retrieval chain
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_store.as_retriever(search_kwargs={"k": 5}),
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )
    
    # Store chain in user session
    cl.user_session.set("qa_chain", qa_chain)
    
    await cl.Message(content="ì¤€ë¹„ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.").send()

@cl.on_message
async def on_message(message: cl.Message):
    # Get question from user message
    question = message.content
    
    # Get chain from user session
    qa_chain = cl.user_session.get("qa_chain")
    if not qa_chain:
        await cl.Message(content="ì„¸ì…˜ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œê³ ì¹¨ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.").send()
        return
    
    # Show thinking message
    thinking_msg = cl.Message(content="ìƒê° ì¤‘...")
    await thinking_msg.send()
    
    # Process query
    try:
        with cl.Step("ë…¼ë¬¸ ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± ì¤‘...") as step:
            response = qa_chain({"query": question})
            step.output = "ì‘ë‹µ ìƒì„± ì™„ë£Œ"
    except Exception as e:
        # Handle error - instead of updating, send a new message
        await cl.Message(content=f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}").send()
        return
    
    # Instead of updating, send a new message with the result
    await cl.Message(content=response["result"]).send()
    
    # Display source documents
    sources_text = "### ì°¸ê³  ë…¼ë¬¸\n\n"
    for i, doc in enumerate(response["source_documents"]):
        sources_text += f"**{i+1}.** ë…¼ë¬¸ ID: {doc.metadata.get('paper_id', 'N/A')}\n"
        sources_text += f"   ì œëª©: {doc.metadata.get('title', 'N/A')}\n"
        sources_text += f"   ì €ë„: {doc.metadata.get('journal', 'N/A')}\n\n"
    
    # Send sources as a new message
    await cl.Message(content=sources_text).send()